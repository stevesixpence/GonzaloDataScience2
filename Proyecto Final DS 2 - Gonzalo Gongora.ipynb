{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto Final - Data Science 2**\n",
    "## **Prediccion de Pais Destino en AirBnB**\n",
    "\n",
    "En esta entrega se cubren las siguentes rubricas:\n",
    "\n",
    "1. Generar preguntas o hipótesis de interés sobre el dataset elegido para el proyecto final.\n",
    "2. Crear gráficos que usen al menos tres variables y hacer un diagnóstico de lo que los mismos nos están diciendo.\n",
    "3. Vincular los gráficos y análisis numéricos hechos hasta el momento con las preguntas de interés.\n",
    "4. Identificar valores perdidos.\n",
    "\n",
    "El dataset esta compuesto de una carpeta de archivos en formato CSV conteniendo informacion de usuarios de USA que realizan reservaciones en AirBnB para diferentes paises. Dicho Dataset puede descargarse de kaggle (solo despues de un muy engorroso proceso de alta de usuario) en la siguiente liga:\n",
    "\n",
    "__https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings/data__\n",
    "\n",
    "El sitio web describe el Dataset con el siguiente parrafo:\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "Dataset Description'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "In this challenge, you are given a list of users along with their demographics, web session records, and some summary statistics. You are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA.\n",
    "\n",
    "There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. Please note that 'NDF' is different from 'other' because 'other' means there was a booking, but is to a country not included in the list, while 'NDF' means there wasn't a booking.\n",
    "\n",
    "The training and test sets are split by dates. In the test set, you will predict all the new users with first activities after 7/1/2014 (note: this is updated on 12/5/15 when the competition restarted). In the sessions dataset, the data only dates back to 1/1/2014, while the users dataset dates back to 2010. ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pt. I Analisis de Metadatos\n",
    "\n",
    "El dataset consta de 5 archivos en formato CSV que contienen unformacion de los usuarios y reservaciones en la plataforma AirBnb. Los archivos son los siguientes:\n",
    "\n",
    "1. **age_gender_bkts**: contiene info. de la edad, pais, genero, ID y anyo de registro de los usuarios\n",
    "2. **countries**: describe los paises destino de los diferentes usuarios, describe la ubicacion con latitud y longitud, distancia en km, descripcion del destino en km2, lenguaje del pais destino y distancia levenshtein del lenguaje\n",
    "3. **sessions**: contiene informacion de sesion del usuario, userID, accion ejecutada, tipo de accion, detalle de accion, tipo de dispositivo y segundos transcurridos\n",
    "4. **test_users**: dataset de 15 columnas con informacion de los regtistros como user ID, fecha de creacion del usuario, timestamp de actividad, fecha de la primera reserva, genero, edad, metodo de registro, flujo de registro, idioma, canal afiliado, proveedor afiliado, primer afiliado reconocido, tipo de aplicacion de registro y primer browser. Se usa para probar el modelo predictivo.\n",
    "5. **train_users_2**: dataset de 16 columnas con informacion de los regtistros como user ID, fecha de creacion del usuario, timestamp de actividad, fecha de la primera reserva, genero, edad, metodo de registro, flujo de registro, idioma, canal afiliado, proveedor afiliado, primer afiliado reconocido, tipo de aplicacion de registro, primer browser y pais de destino. \n",
    "\n",
    "El objetivo de todo el dataset es que el programador se dedique a disenyar y entrenar un modelo predictivo para lograr saber cual es el siguiente pais que visitara un usuario registrado.\n",
    "\n",
    "Los dos archivos de Excel relevantes para el estudio (a mi consideracion) son:\n",
    "\n",
    "1. **train_set**\n",
    "2. **test_set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias Necesarias\n",
    "Este proyecto empieza importando las librerias necesarias. Necesito bastantes librerias para llenar, organizar, procesar, y revisar estructuras de datos o Dataframes.\n",
    "\n",
    "Procesamiento de Dataframes:\n",
    "\n",
    "    1. Pandas\n",
    "    2. NumPy\n",
    "    3. datetime  \n",
    "\n",
    "Generacion de Graficas:\n",
    "\n",
    "    4. Matplotlib\n",
    "\n",
    "Modelado de Machine Learning:\n",
    "\n",
    "    5. sklearn LabelEncoder\n",
    "    6. sklearn train_test_slpit\n",
    "    7. sklearn RandomForestClassifier\n",
    "    8. sklearn cross_val_score\n",
    "    9. sklearn classification_report\n",
    "    10. sklearn confusion_matrix,mean_absolute_error, mean_squared_error, r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Empiezo a importar las librerias necesarias para el proyecto\n",
    "import matplotlib.pyplot as plt # Para graficar\n",
    "import pandas as pd  # Para manipulacion de datos\n",
    "import numpy as np   # Para operaciones numericas\n",
    "import seaborn as sns\n",
    "import lazypredict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando Datasets...\n",
    "train_df = pd.read_csv(\"train_users_2.csv\")\n",
    "test_df = pd.read_csv(\"test_users.csv\")\n",
    "\n",
    "# Mostrar informacion basica. Si deseo Mergearlos, necesito que sus vectores tengan la misma forma (1x16 , 1x15)\n",
    "print(\"Train Shape:\", train_df.shape)\n",
    "print(\"Test Shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestro el nombre de todas las columnas disponibles en los dataframes, \n",
    "print(\"Train columns:\\n\", train_df.columns)\n",
    "print(\"\\nTest columns:\\n\", test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrain Data Types:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest Data Types:\\n\", test_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion de 'train_set'\n",
    "Es un vector Train Shape: (213451, 16)\n",
    "Tengo 16 columnas:\n",
    "\n",
    "    1. id                        9. language\n",
    "    2. date_account_created      10. affiliate_channel  \n",
    "    3. timestamp_first_active    11. affiliate_provider\n",
    "    4. date_first_booking        12. first_affiliate_tracked\n",
    "    5. gender                    13. signup_app\n",
    "    6. age                       14. first_device_type\n",
    "    7. signup_method             15. first_browser\n",
    "    8. signup_flow               16. country_destination\n",
    "\n",
    "Existen datos raros a simple vista?\n",
    "\n",
    "### Descripcion de 'test_set'\n",
    "Cuantas filas x columnas? Test Shape: (62096, 15)\n",
    "Tengo 16 columnas:\n",
    "\n",
    "    1. id                        9. language\n",
    "    2. date_account_created      10. affiliate_channel  \n",
    "    3. timestamp_first_active    11. affiliate_provider\n",
    "    4. date_first_booking        12. first_affiliate_tracked\n",
    "    5. gender                    13. signup_app\n",
    "    6. age                       14. first_device_type\n",
    "    7. signup_method             15. first_browser\n",
    "    8. signup_flow               \n",
    "Existen datos raros a simple vista?\n",
    "\n",
    "\n",
    "Desde el vamos, el test_set carece de la columna Target, que seria 'country_destination'. Es decir: \n",
    "\n",
    "**No puedo hacer merge a estos datasets para obtener mas info para el entrenamiento de mi modelo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificacion de Ruido en el Dataset\n",
    "\n",
    "Empiezo buscando datos faltantes, nulos, corruptos, etc en ambos datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values in Train Set:\\n\", train_df.isnull().sum())\n",
    "print(\"\\nMissing Values in Test Set:\\n\", test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=213451 #Numero de filas en el dataset de entrenamiento\n",
    "b= 100\n",
    "c=train_df['date_first_booking'].isnull().sum()\n",
    "x = (b*c)/a \n",
    "print(\"Perdida de  un \", x ,\" % de los datos\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= 213451 #Numero de filas en el dataset de entrenamiento\n",
    "e= 100\n",
    "f= (train_df['country_destination']== 'NDF').sum()\n",
    "y = (f*e)/d \n",
    "print(\"Perdida de  un \", y ,\" % de los datos\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estoy notando que en el archivo 'train_users_2' muchos usuarios NO tienen fecha de primera reserva. \n",
    "\n",
    "Como dato curioso, el dataset de entrenamiento tiene un total de 213451 filas, de las cuales:\n",
    "\n",
    "    1. No existe ninguna fila en 'country_destination' que tenga valor Nulo y/o NaN\n",
    "    2. Existen 124543 filas en 'first_date_booking' con valores Nulos y/o NaN\n",
    "    3. Existen 124543 filas en 'country_destination' con valores 'NDF' o sin pais definido.\n",
    "\n",
    "Si echo un vistazo en Excel y ordeno el dataset de menor a mayor fecha, todos los NDF coinciden con los registros que corresponden a 'first_date_booking' vacios\n",
    "\n",
    "Como el objetivo de mis predicciones es conocer el destino de un usuario que viaja, podria droppear las filas de usuarios que NO tienen una reserva y observar si los resultados son buenos al momento de observar la distribucion. ---> Si este fuera el caso, **perderia 58.3473% de los datos**\n",
    "\n",
    "En ese caso, decido dejar estos datos y compensarlo posteriormente con algun metodo ya sea undersampling, oversampling, class weights o afines.\n",
    "\n",
    "Tambien observo que la columna 'age' tiene datos faltantes, sin embargo estos los voy a conservar, pues los puedo llenar con una mediana para no perder datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_summary = {col: train_df[col].nunique() for col in categorical_columns}\n",
    "\n",
    "print(\"\\nUnique Values in Categorical Features:\\n\", categorical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAge Summary:\\n\", train_df['age'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['timestamp_first_active'] = pd.to_datetime(train_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "test_df['timestamp_first_active'] = pd.to_datetime(test_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "print(\"\\nConverted Timestamp Example:\\n\", train_df[['timestamp_first_active']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of categorical features\n",
    "categorical_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_summary = {col: train_df[col].nunique() for col in categorical_columns}\n",
    "\n",
    "# Unique values in target variable\n",
    "target_distribution = train_df['country_destination'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Checking age distribution and potential outliers\n",
    "age_summary = train_df['age'].describe()\n",
    "\n",
    "# Convert timestamp_first_active to datetime format\n",
    "train_df['timestamp_first_active'] = pd.to_datetime(train_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "test_df['timestamp_first_active'] = pd.to_datetime(test_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Display results\n",
    "categorical_summary, target_distribution, age_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos con el Analisis de Metadatos de mi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Findings:\n",
    "\n",
    "    Dataset Sizes\n",
    "        Train set: 213,451 rows × 16 columns\n",
    "        Test set: 62,096 rows × 15 columns (missing country_destination, which is expected)\n",
    "\n",
    "    Column Overview\n",
    "        The datasets share the same structure except for country_destination in the train set.\n",
    "        Most columns are categorical, with age as a numerical feature.\n",
    "        timestamp_first_active is stored as an integer (likely a timestamp).\n",
    "\n",
    "    Missing Values\n",
    "        date_first_booking has 124,543 missing values (58%) in the train set and completely missing (100%) in the test set.\n",
    "        age has 87,990 missing values (41%) in the train set and 28,876 missing (46%) in the test set.\n",
    "        first_affiliate_tracked has some missing values (6,065 in train, 20 in test).\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "    Check value distributions and unique categories.\n",
    "    Investigate age (e.g., invalid values or outliers).\n",
    "    Convert timestamp_first_active to a readable date.\n",
    "    Examine the target variable country_destination.\n",
    "\n",
    "I'll continue the analysis now. ​\n",
    "\n",
    "​\n",
    "Key Insights:\n",
    "\n",
    "    Categorical Feature Overview\n",
    "        gender: 4 unique values\n",
    "        signup_method: 3 unique values\n",
    "        language: 25 unique values\n",
    "        first_browser: 52 unique values (could be simplified)\n",
    "        country_destination: 12 unique values (our target variable)\n",
    "\n",
    "    Target Variable Distribution\n",
    "        58.3% of users did not book any destination (NDF)\n",
    "        29.2% booked in the US\n",
    "        Other countries have much lower representation (e.g., France: 2.35%, Italy: 1.33%)\n",
    "        The dataset is highly imbalanced, which may require handling techniques (e.g., resampling or weighted loss functions).\n",
    "\n",
    "    Age Distribution and Outliers\n",
    "        Mean age: 49.67, but the max value is 2014, which is clearly incorrect.\n",
    "        75% of users are 43 or younger, suggesting values above ~100 might be invalid.\n",
    "        Likely need to filter or impute extreme age values.\n",
    "\n",
    "    Timestamp Conversion\n",
    "        timestamp_first_active is now a readable datetime format, which will allow extracting useful features (e.g., year, month, weekday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pt. 2 EDA\n",
    "\n",
    "Se realiza el tratamiento de los datos con el fin de mejorar la prediccion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizacion de Columna 'age'\n",
    "\n",
    "Comienzo graficando los valores de la columna 'age' para conocer la distribucion de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = train_df['age']\n",
    "print(train_df['age'].describe())\n",
    "\n",
    "age.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He decidido quitar los valores ilogicos de la columa age (por ejemplo, una persona de 2024 a;os de edad). Considero que para este analisis vale la pena pensar que las personas registradas deben tener al menos 18 a;os por motivos de politicas de la plataforma, y maximo 100 a;os como limite superior.\n",
    "\n",
    "Hago asi para los conjuntos de test y train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp_first_active to datetime format\n",
    "train_df['timestamp_first_active'] = pd.to_datetime(train_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Clean 'age' column: Keep values between 18 and 100, set others to NaN\n",
    "valid_age_range_test = (train_df['age'] >= 18) & (train_df['age'] <= 100)\n",
    "train_df.loc[~valid_age_range_test, 'age'] = np.nan\n",
    "\n",
    "age.value_counts().plot.bar()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(age.dropna(), bins=15, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aget = test_df['age']\n",
    "aget.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp_first_active to datetime format\n",
    "test_df['timestamp_first_active'] = pd.to_datetime(test_df['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Clean 'age' column: Keep values between 18 and 100, set others to NaN\n",
    "valid_age_range_test = (test_df['age'] >= 18) & (test_df['age'] <= 100)\n",
    "test_df.loc[~valid_age_range_test, 'age'] = np.nan\n",
    "\n",
    "aget.value_counts().plot.bar()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(aget, bins=15, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizacion de Columna 'gender'\n",
    "\n",
    "Grafico la columna de Genero del dataset, pues considero que esta variable puede ayudar a realizar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se grafican los valores unicos en la columna de genero, pues podria ser un buen indicador:\n",
    "train_df['gender'].value_counts().plot.bar()\n",
    "gender = train_df['gender']\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(gender, bins=15, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderc = train_df['gender'].replace(\"-unknown-\", np.nan)\n",
    "gender.value_counts().plot.bar()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(genderc.dropna(), bins=15, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizacion de Columna 'signup_flow'\n",
    "\n",
    "Esta columna podria ser util para el modelo predictivo, decido graficarla para observar la distribucion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = train_df['signup_flow']\n",
    "sign.value_counts().plot.bar()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sign, bins=15, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'language'\n",
    "\n",
    "Podria ser que el lenguaje del usuario de indicios sobre los paises que quiera visitar, posiblemente por razones culturales del lugar de origen. Pongamoslo a prueba.\n",
    "\n",
    "Esta variable es multiple categorica, por lo que un histograma con su distribucion no seria util para saber que tan bien va a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = train_df['language']\n",
    "lang.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, la inmensa mayoria de los usuarios es angloparlante, por lo cual no creo encontrar mucha informacion con estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'country_destination'\n",
    "\n",
    "Es mi columna objetivo para las predicciones, por lo cual debera ser manejada con mucho cuidado. Empezare por observar su histograma para determinar la distribucion de estos datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Check for missing values (though there shouldn't be any in the training set).\n",
    "    Look for inconsistencies (e.g., unexpected values).\n",
    "    Analyze class distribution again (since it's highly imbalanced).\n",
    "    Decide on encoding:\n",
    "        We’ll likely use label encoding (mapping each country to a number) for modeling.\n",
    "        Alternatively, one-hot encoding could be used if the model requires categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = train_df['country_destination']\n",
    "destination.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoria de los datos son a un pais NDF. Como he explorado antes, no puedo recortar los datos NDF, pues corresponden al 58.3~% de los datos del dataset. La mayoria de los usuarios cayendo en las categorias NDF y US. Esto tendra que ser compensado mas adelante en el proyecto con alguna medida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in 'country_destination':\", train_df['country_destination'].isnull().sum())\n",
    "\n",
    "# Check unique values\n",
    "print(\"\\nUnique values in 'country_destination':\", train_df['country_destination'].unique())\n",
    "\n",
    "# Check target variable distribution\n",
    "target_distribution = train_df['country_destination'].value_counts(normalize=True) * 100\n",
    "print(\"\\nTarget variable distribution (%):\\n\", target_distribution)\n",
    "\n",
    "#target_distribution.plot()\n",
    "plt.hist(target_distribution,15) ## Antes de graficar tengo que hacer hot encoding\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estoy intentando observar la distribucion de una variable Categorica, lo cual no llegara muy lejos pues la distribucion aplica para valores numericos y continuos, no para categoricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pt. 3  Data Wrangling\n",
    "\n",
    "En este apartado voy a procurar obtener datos con una distribucion mas cercana a lo que se considera Normal y dejarlos lo mas limpios posible para proponer un modelo predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'age'\n",
    "\n",
    "Procedo a tratar los datos del dataset de Entrenamiento imputando la mediana de edad en los valores vacios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing ages with the same median from the training set (34)\n",
    "median_age = 34  # Use the calculated median from train data\n",
    "train_df['agef'] = train_df['age']\n",
    "\n",
    "#Aqui sobreescribo la columna con datos imputados sobre la original\n",
    "train_df['agef'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Verify the cleaned age distribution\n",
    "print(train_df['agef'].describe())\n",
    "valid_age_range_test = (train_df['agef'] >= 18) & (train_df['agef'] <= 100)\n",
    "train_df.loc[~valid_age_range_test, 'agef'] = np.nan\n",
    "train_df['agef'].value_counts().plot.bar()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(train_df['agef'].dropna(), bins=15, color='blue', alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['age'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['agef'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de intentar reemplazar los valores NaN con la mediana de Edad, simplemente obtenemos un pico muy pronunciado con la distribucion aun cargada a la izquierda.\n",
    "\n",
    "Intentare reiniciar mis valores y realizar un Sampleo Aleatorio (random sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract valid (non-null) ages\n",
    "valid_ages = train_df['age'].dropna()\n",
    "\n",
    "# Apply random sampling for missing values\n",
    "train_df['age_r']=train_df['age'].apply(lambda x: np.random.choice(valid_ages) if np.isnan(x) else x)\n",
    "train_df['age_r'].describe()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(train_df['age_r'], bins=15, color='blue', alpha=0.7)\n",
    "\n",
    "balanced_df=train_df\n",
    "\n",
    "balanced_df['age']=train_df['age_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['age_r'].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'gender'\n",
    "\n",
    "Considerando que respeto los valores 'male', 'female' y 'other' como variables categoricas, tengo que balancearlas de otra manera.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X = train_df.drop(columns=['gender'])\n",
    "y = train_df['gender']\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "balanced_df = X_resampled.copy()\n",
    "\n",
    "## Guardo y sobreescribo la columna balanceada sobre la original\n",
    "\n",
    "balanced_df['gender'] = y_resampled\n",
    "\n",
    "y_resampled.head()\n",
    "y_resampled.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['gender'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'language'\n",
    "\n",
    "Decido emplear RandomUnderSampling para balancear el sesgo pesado que los datos tienen hacia el idioma Ingles o 'en', en la columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X = train_df.drop(columns=['language'])\n",
    "y = train_df['language']\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "balanced_df = X_resampled.copy()\n",
    "\n",
    "## Guardo y sobreescribo la columna balanceada sobre la original\n",
    "\n",
    "balanced_df['language'] = y_resampled\n",
    "\n",
    "y_resampled.head()\n",
    "y_resampled.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['language'].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna 'Country Destination'\n",
    "\n",
    "Empiezo a hacer el balanceo de la columna objetivo, empleando RandomUndersampling y dejar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = train_df.drop(columns=['country_destination'])\n",
    "y = train_df['country_destination']\n",
    "\n",
    "# Apply undersampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "# Recreate a balanced dataframe\n",
    "balanced_df = pd.DataFrame(X_resampled, columns=X.columns)  # Reconstruct X as a DataFrame\n",
    "balanced_df['country_destination'] = y_resampled  # Add the target column back\n",
    "\n",
    "# Check the class distribution\n",
    "y_resampled.value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['country_destination'].tail(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Balanceado\n",
    "\n",
    "Al final de cada columna procesada he estado guardando los datos \"limpios\" en las columnas de un dataframe nuevo llamado \"balanced_df\", que es el que voy a usar para mi procesamiento del modelo predictivo.\n",
    "Empiezo quitando las columnas que no voy a usar para el analisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.drop(columns=['id','age_r' ], inplace=True)\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Correlaciones\n",
    "\n",
    "Se crea una matriz de correlaciones para determinar cuales son las Features que podemos usar para predecir y cuales podemos dejar de un lado para optimizar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to numerical encoding\n",
    "balanced_df['country_destination'] = balanced_df['country_destination'].astype('category').cat.codes\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = balanced_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Compute correlation with target\n",
    "correlation = balanced_df[numerical_features + ['country_destination']].corr()['country_destination'].sort_values(ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Convert target variable to numerical encoding\n",
    "train_df['country_destination'] = train_df['country_destination'].astype('category').cat.codes\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Compute correlation with target\n",
    "correlation = train_df[numerical_features + ['country_destination']].corr()['country_destination'].sort_values(ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparado con el dataset original, podemos observar que la correlacion en las columnas del data set balanceado ha mejorado, aunque sea en poca medida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pt. 4 - Diseño del Modelo Predictivo\n",
    "\n",
    "En esta parte procedo a integrar modelos predictivos dise;ados para mi problema.\n",
    "\n",
    "Tengo un problema: \n",
    "                    De aprendizaje supervisado: Conozco mi variable objetivo\n",
    "                    Clasificatorio: Tengo que determinar en que categoria va a caer un usuario que viaja en AirBnB.\n",
    "                    Multiclase: La variable objetivo se divide en 12 categorias distintas: AU, CA, DE, ES, FR, GB, IT, NL, PT, US, other, NDF.\n",
    "                \n",
    "En terminos de datos, existe la siguiente dificultad:\n",
    "\n",
    "         El dataset 'test_users' NO posee una columna 'country_destination'.\n",
    "\n",
    "Por lo cual tengo que apartar esos datos y particionar los datos en el conjunto de 'train_users_2' para poder entrenar un Modelo Predictivo y Validar dichas predicciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionando el Dataset 'train_users_2'\n",
    "\n",
    "Decido hacer un particionado estratificado con un estado aleatorio predeterminado, para eso utilizare metodos en la libreria 'sklearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quiero hacer un modelo predictivo usando mis datos ya procesados, son los siguientes:\n",
    "\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Separate features and target\n",
    "X = balanced_df.drop(columns=['country_destination'])  # Replace 'target_column_name' with the actual name\n",
    "y = balanced_df['country_destination']\n",
    "\n",
    "# Split the train set into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento y Encoding de Datos Categoricos\n",
    "# Convert date columns to datetime\n",
    "\n",
    "date_cols = ['date_account_created', 'timestamp_first_active', 'date_first_booking']\n",
    "for col in date_cols:\n",
    "    X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "    X_val[col] = pd.to_datetime(X_val[col], errors='coerce')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical features from dates\n",
    "for df in [X_train, X_val]:\n",
    "    df['dac_year'] = df['date_account_created'].dt.year\n",
    "    df['dac_month'] = df['date_account_created'].dt.month\n",
    "    df['dac_day'] = df['date_account_created'].dt.day\n",
    "    \n",
    "    df['tfa_year'] = df['timestamp_first_active'].dt.year\n",
    "    df['tfa_month'] = df['timestamp_first_active'].dt.month\n",
    "    df['tfa_day'] = df['timestamp_first_active'].dt.day\n",
    "    \n",
    "    df['dfb_year'] = df['date_first_booking'].dt.year.fillna(0)  # Fill NaN with 0\n",
    "    df['dfb_month'] = df['date_first_booking'].dt.month.fillna(0)\n",
    "    df['dfb_day'] = df['date_first_booking'].dt.day.fillna(0)\n",
    "\n",
    "# Drop original date columns\n",
    "X_train.drop(columns=date_cols, inplace=True)\n",
    "X_val.drop(columns=date_cols, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = balanced_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "## Se usa LabelEncoding para variables categoricas con multiples etiquetas\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    balanced_df[col] = encoder.fit_transform(balanced_df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = balanced_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale numerical features\n",
    "balanced_df[numerical_cols] = scaler.fit_transform(balanced_df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = balanced_df.drop(columns=['country_destination'])\n",
    "y = balanced_df['country_destination']\n",
    "\n",
    "# Split data into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit models\n",
    "models, predictions = clf.fit(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Display model performance\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el top 3 de algoritmos (ordenados de mayor eficiencia a menor), son:\n",
    "\n",
    "                                    Accuracy    Balanced Accuracy  ROC AUC   F1 Score\n",
    "    1. BaggingClassifier                  0.20               0.20    None      0.20   \n",
    "    2. DecisionTreeClassifier             0.20               0.20    None      0.20   \n",
    "    3. RandomForestClassifier             0.18               0.18    None      0.18     \n",
    "\n",
    "Sin embargo, estos porcentajes de precision son horribles. Procedo a intentar otro algoritmo con otro tipo de balanceo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intentando XGBoost\n",
    "\n",
    "Adapto el dataset balanceado a un formato que puede leer XGBoost:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas de XGBoost con el Dataset recortado\n",
    "\n",
    "Voy a probar este algoritmo con el Dataset Balanceado que he generado, cabe mencionar que recorte algunas columnas para intentar mejorar el tiempo de procesamiento y eliminar el ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = balanced_df\n",
    "# Preprocessing\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=['country_destination'])\n",
    "\n",
    "# Handle missing values in features\n",
    "data = data.fillna(-1)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ['gender', 'language']\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature].astype(str))\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "data['country_destination'] = le.fit_transform(data['country_destination'])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination'], axis=1)\n",
    "y = data['country_destination']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier(objective='multi:softprob', num_class=len(le.classes_), random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# If you want to see the predicted classes in their original form\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(y_pred_original[:10])  # Print the first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de XGBoost con pocos datos\n",
    "\n",
    "El resultado de la validacion para este conjunto de datos \n",
    "        'balanced_df'\n",
    "Fue de un vergonzoso 10.36%. Por lo cual podemos volver a concluir:\n",
    "\n",
    "                        \"En Data Science, cuanto mas tengas, mejor.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba XGBoost con Dataset con mas columnas\n",
    "\n",
    "Ahora intento el mismo algoritmo, pero sin amputar tanta informacion, pues los resultados previos fueron vergonzosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('train_users_2.csv')\n",
    "# Preprocessing\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=['country_destination'])\n",
    "\n",
    "# Handle missing values in features\n",
    "data = data.fillna(-1)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ['gender', 'signup_method', 'signup_app', 'first_device_type', 'first_browser', \n",
    "                        'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'language']\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature].astype(str))\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "data['country_destination'] = le.fit_transform(data['country_destination'])\n",
    "\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(['id', 'date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination'], axis=1)\n",
    "y = data['country_destination']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier(objective='multi:softprob', num_class=len(le.classes_), random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# If you want to see the predicted classes in their original form\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(y_pred_original[:10])  # Print the first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Voy a respetar la mayor parte de las columnas posible en vez de droppear las que empiricamente se quitarian\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('train_users_2.csv')\n",
    "\n",
    "# Preprocessing\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=['country_destination'])\n",
    "\n",
    "### Voy a droppear valores NaN de la columna date_fist_booking\n",
    "# data = data.dropna(subset=['date_first_booking'])\n",
    "### Resulta en un bajon de Accuracy, baja a un 69.9%\n",
    "\n",
    "# Handle missing values in features\n",
    "data = data.fillna(-1)\n",
    "\n",
    "# Feature Engineering for date columns\n",
    "# Convert to datetime\n",
    "data['date_account_created'] = pd.to_datetime(data['date_account_created'])\n",
    "data['timestamp_first_active'] = pd.to_datetime(data['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "data['date_first_booking'] = pd.to_datetime(data['date_first_booking'])\n",
    "\n",
    "# Extract features from date_account_created\n",
    "data['account_created_year'] = data['date_account_created'].dt.year\n",
    "data['account_created_month'] = data['date_account_created'].dt.month\n",
    "data['account_created_day'] = data['date_account_created'].dt.day\n",
    "data['account_created_weekday'] = data['date_account_created'].dt.weekday  # Monday=0, Sunday=6\n",
    "\n",
    "# Extract features from timestamp_first_active\n",
    "data['first_active_year'] = data['timestamp_first_active'].dt.year\n",
    "data['first_active_month'] = data['timestamp_first_active'].dt.month\n",
    "data['first_active_day'] = data['timestamp_first_active'].dt.day\n",
    "data['first_active_hour'] = data['timestamp_first_active'].dt.hour\n",
    "\n",
    "# Extract features from date_first_booking (if available)\n",
    "data['booking_year'] = data['date_first_booking'].dt.year\n",
    "data['booking_month'] = data['date_first_booking'].dt.month\n",
    "data['booking_day'] = data['date_first_booking'].dt.day\n",
    "data['booking_weekday'] = data['date_first_booking'].dt.weekday\n",
    "\n",
    "# Calculate time differences\n",
    "data['time_to_first_booking'] = (data['date_first_booking'] - data['date_account_created']).dt.days\n",
    "data['time_since_account_created'] = (pd.Timestamp.now() - data['date_account_created']).dt.days\n",
    "\n",
    "# Drop the original date columns \n",
    "data = data.drop(['date_account_created', 'timestamp_first_active', 'date_first_booking'], axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "## Procuro mantener la mayor cantidad de columnas posibles\n",
    "categorical_features = ['gender', 'signup_method', 'signup_app', 'first_device_type', 'first_browser', \n",
    "                        'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'language']\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature].astype(str))\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "data['country_destination'] = le.fit_transform(data['country_destination'])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(['id', 'country_destination'], axis=1)  # Drop 'id' and the target column\n",
    "y = data['country_destination']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier(objective='multi:softprob', num_class=len(le.classes_), random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# If you want to see the predicted classes in their original form\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(y_pred_original[:10])  # Print the first 10 predictions\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 6],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_train)\n",
    "test_accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_train)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "conf_matrix = confusion_matrix(y_train, y_pred)\n",
    "precision = precision_score(y_train, y_pred, average='weighted')  # Use 'macro' for class imbalance\n",
    "recall = recall_score(y_train, y_pred, average='weighted')\n",
    "f1 = f1_score(y_train, y_pred, average='weighted')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de Features\n",
    "\n",
    "Procedo a graficar las variables de fechas y determinar cuanto impactan en el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar en estos datos, aquellos que tienen mas peso son los de 'Edad'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados XGBoost\n",
    "Usando XGBoost con la mayor cantidad de columnas posibles y sin amputar datos, obtengo una \n",
    "\n",
    "                                            Accuracy de 87.5%\n",
    "\n",
    "\n",
    "Empleando GridSearch para ajustar hiperparametros, el tiempo de procesamiento es muchisimo mas alto. \n",
    "Y el modelo mejora hasta un... \n",
    "\n",
    "                                            Accuracy de 87.579%\n",
    "\n",
    "Lo cual representa una mejora, sin embargo no considero que el tiempo de procesamiento amerite el incremento en Accuracy.\n",
    "\n",
    "Finalmente, corroborando el modelo que acabo de realizar, se obtienen las siguientes metricas.\n",
    "\n",
    "✓ Accuracy: 0.8758\n",
    "✓ Precision: 0.7887\n",
    "✓ Recall: 0.8758\n",
    "✓ F1 Score: 0.8246\n",
    "\n",
    "Los cuales aseguran que el modelo predictivo es hasta cierto punto confiable:\n",
    "El modelo aprendió patrones fuertes en los datos y tiene una alta capacidad de predicción asi un buen balance entre precisión y recall. Sin embargo, al tener una precisión más baja que recall, puede estar prediciendo destinos erróneos en algunos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "1. Comparando el performance de XGBoost sin eliminar tantas filas (train_users_2.csv)\n",
    "vs el dataset recortado (balanced_df), el dataset con mas informacion tiene muchisima mejor precision que el balanceado. Mientras mas informacion respete, mejor.\n",
    "2. Lazypredict ayuda a probar multiples metodos de machine learning y evaluar su eficiencia de una manera muy conveniente, tomando en cuenta que el usuario no tiene que escribir tantas lineas de codigo.\n",
    "3. El balanceo de informacion es importante, pero si quitas muchas columnas, terminaras con un modelo PESIMO.\n",
    "4. De nuevo, en el mundo de ciencia de datos: a mayor cantidad, mejor. Claro, que la limpieza de los datos es importante, pero si tienes muchisimos datos, tienes mas posibilidades de hallar algo bueno.\n",
    "5. La afinacion de hiperparametros puede ser muy util para mejorar el rendimiento del modelo, pero a veces es tan pequenya la mejoria, que no vale la pena aumentar el tiempo de procesamiento.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
